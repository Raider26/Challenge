# üèÉ Text‚ÄìMotion Retrieval ‚Äî Data Challenge

> √âtant donn√© une description textuelle (*"a person walks forward and sits down"*), retrouver parmi N motions candidates celle qui correspond.

---

## Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Structure du projet](#structure-du-projet)
3. [Installation](#installation)
4. [Format des donn√©es](#format-des-donn√©es)
5. [Architecture du mod√®le](#architecture-du-mod√®le)
6. [Pipeline complet](#pipeline-complet)
7. [Entra√Ænement](#entra√Ænement)
8. [√âvaluation](#√©valuation)
9. [Pr√©diction & Soumission](#pr√©diction--soumission)
10. [Hyperparam√®tres](#hyperparam√®tres)
11. [R√©sultats & Visualisation](#r√©sultats--visualisation)

---

## Vue d'ensemble

Le probl√®me est un **retrieval cross-modal** : aligner un espace textuel et un espace de mouvements humains dans un espace vectoriel commun, de fa√ßon √† pouvoir mesurer leur similarit√© par produit scalaire.

```
Texte  ‚îÄ‚îÄ[CLIP text encoder]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                        ‚îú‚îÄ‚îÄ‚ñ∫ cosine similarity ‚îÄ‚îÄ‚ñ∫ ranking
Motion ‚îÄ‚îÄ[MotionEncoder (Transformer 1D)]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Les deux encodeurs projettent dans le **m√™me espace de dimension 512**. La loss **InfoNCE** les force √† aligner les paires correctes (texte_i ‚Üî motion_i) tout en √©loignant les paires incorrectes.

---

## Structure du projet

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ motions/          ‚Üê fichiers .npy  (shape : T √ó 384)
‚îÇ   ‚îú‚îÄ‚îÄ texts/            ‚Üê fichiers .txt  (1 √† 3 descriptions par motion)
‚îÇ   ‚îú‚îÄ‚îÄ train.txt         ‚Üê liste des noms de fichiers d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ val/              ‚Üê batches de validation g√©n√©r√©s automatiquement
‚îÇ       ‚îú‚îÄ‚îÄ 1/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ text.txt
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ motion_1.npy
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îú‚îÄ‚îÄ gt.csv        ‚Üê v√©rit√©s terrain
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ text-motion_retrieval.ipynb
‚îú‚îÄ‚îÄ motion_encoder.pt           ‚Üê sauvegarde du MotionEncoder apr√®s entra√Ænement
‚îú‚îÄ‚îÄ clip_model_finetuned.pt     ‚Üê sauvegarde de CLIP fine-tun√©
‚îú‚îÄ‚îÄ submission.csv              ‚Üê fichier de soumission final
‚îî‚îÄ‚îÄ tsne_latent_space.png       ‚Üê visualisation de l'espace latent
```

---

## Installation

```bash
pip install torch torchvision open-clip-torch diffusers \
            scikit-learn matplotlib pandas tqdm info-nce
```

> **GPU recommand√©.** Le code fonctionne aussi en CPU mais l'entra√Ænement sera tr√®s lent.

---

## Format des donn√©es

### Motions
Chaque fichier `.npy` contient un array NumPy de shape `(T, 384)` :
- `T` = nombre de frames (variable selon la s√©quence)
- `384` = vecteur de pose par frame, encodant les positions et rotations des articulations des **2 personnes** (format HumanML3D multi-personne)

```python
motion = np.load('data/motions/00001.npy')
# motion.shape ‚Üí (184, 384)
```

La motion encode pour chaque personne :
- `22 √ó 3` positions 3D des joints
- `21 √ó 6` rotations des joints (repr√©sentation 6D)
- vitesses et positions globales

### Textes
Chaque fichier `.txt` contient **1 √† 3 descriptions** de la motion correspondante, une par ligne :

```
a person walks forward and sits down on a chair.
the human walks ahead and takes a seat.
someone walks to a chair and sits.
```

---

## Architecture du mod√®le

### Encodeur texte : CLIP (ViT-B/32)

CLIP est pr√©-entra√Æn√© sur des milliards de paires texte-image. Son encodeur textuel comprend naturellement le langage d√©crivant des actions physiques.

- **Partie visuelle** : gel√©e (inutilis√©e, √©conomise la m√©moire)
- **Partie textuelle** : fine-tun√©e √† `lr √ó 0.1` pour s'adapter au vocabulaire du mouvement
- **Sortie** : vecteur de dim `512`, normalis√© L2

```python
text_emb = F.normalize(clip_model.encode_text(tokens), dim=-1)  # (B, 512)
```

### Encodeur motion : Transformer 1D

Les motions sont des **s√©quences temporelles**, pas des images. Un Transformer 1D peut faire de l'attention entre les frames, capturant la dynamique du mouvement.

```
(B, T, 384)
    ‚îÇ
    ‚ñº  Linear(384 ‚Üí 512) + positional embedding
(B, T, 512)
    ‚îÇ
    ‚ñº  TransformerEncoder (4 layers, 8 heads)
(B, T, 512)
    ‚îÇ
    ‚ñº  Mean pooling sur la dimension temporelle T
(B, 512)
    ‚îÇ
    ‚ñº  LayerNorm
(B, 512)  ‚îÄ‚îÄ‚ñ∫ normalize L2 ‚îÄ‚îÄ‚ñ∫ embedding final
```

> **Pourquoi pas un VAE image ?** Un `AutoencoderKL` est con√ßu pour des images RGB 2D. Passer une motion en image 2D n'a pas de sens physique : il n'y a aucune localit√© spatiale √† exploiter, et l'encodage d√©truirait la structure temporelle.

---

## Pipeline complet

### 1. Preprocessing des motions (`preprocess_motion`)

```python
def preprocess_motion(motion, max_len=512):
    # Normalisation : mean=0, std=1 par feature
    motion = (motion - motion.mean(0)) / (motion.std(0) + 1e-6)

    # Troncature si T > max_len
    if T >= max_len:
        motion = motion[:max_len]
    # Padding avec des z√©ros si T < max_len
    else:
        pad = np.zeros((max_len - T, F))
        motion = np.vstack([motion, pad])

    return torch.tensor(motion)   # (max_len, 384)
```

### 2. Dataset (`TextMotionDataset`)

Chaque entr√©e du dataset est une paire `(texte, motion)` align√©e par nom de fichier. √Ä chaque acc√®s, **une des 3 descriptions est choisie al√©atoirement** ‚Äî c'est une forme d'augmentation de donn√©es qui am√©liore la g√©n√©ralisation.

```python
# fnames = ['00001', '00002', ...]  ‚Üê noms communs aux .npy et .txt
dataset = TextMotionDataset(fnames, data_root)
# dataset[0] ‚Üí ("a person walks forward...", tensor(512, 384))
```

### 3. DataLoader & collate

La tokenisation CLIP est faite dans la `collate_fn` (plus efficace que dans `__getitem__`) :

```python
def collate_fn(batch):
    texts, motions = zip(*batch)
    text_tokens  = tokenizer(list(texts))     # (B, 77)  ‚Äî 77 tokens max CLIP
    motion_batch = torch.stack(motions)       # (B, 512, 384)
    return text_tokens, motion_batch
```

---

## Entra√Ænement

### Loss : InfoNCE

Pour un batch de N paires, la matrice de similarit√© cosinus est :

```
               motion_1  motion_2  ...  motion_N
  texte_1    [  0.95      0.12     ...   0.08  ]   ‚Üê on veut maximiser la diagonale
  texte_2    [  0.11      0.91     ...   0.05  ]
  ...
  texte_N    [  0.07      0.09     ...   0.88  ]
```

La loss est une **cross-entropy sym√©trique** : elle maximise les valeurs diagonales (paires correctes) et minimise les hors-diagonales (N-1 n√©gatifs implicites par exemple). Plus le batch est grand, plus il y a de n√©gatifs durs ‚Üí apprentissage plus riche. C'est exactement la loss de CLIP.

```python
loss = InfoNCE(temperature=0.07)(text_emb, motion_emb)
```

La temp√©rature `0.07` contr√¥le la nettet√© de la distribution : plus elle est basse, plus la loss est "dure" (p√©nalise fortement les mauvais classements).

### Optimiseur

```python
optimizer = AdamW([
    {'params': clip_model.transformer.parameters(), 'lr': 1e-5},  # CLIP : doucement
    {'params': motion_encoder.parameters(),         'lr': 1e-4},  # Motion enc : normal
])
scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)
```

Le scheduler **cosinus** diminue le learning rate progressivement pour stabiliser la convergence en fin d'entra√Ænement.

### Boucle d'entra√Ænement (r√©sum√©)

```
Pour chaque √©poque :
  Pour chaque batch (text_tokens, motion_batch) :
    1. text_emb   = normalize(CLIP.encode_text(text_tokens))
    2. motion_emb = normalize(MotionEncoder(motion_batch))
    3. loss = InfoNCE(text_emb, motion_emb)
    4. loss.backward() + optimizer.step()
  
  Valider sur val_loader (sans gradient)
  scheduler.step()
  Sauvegarder les mod√®les
```

---

## √âvaluation

### G√©n√©ration des batches de validation

Pour √©valuer localement sans le test set officiel, `generate_val_batches` simule le protocole du challenge :

1. Tire **30 groupes de 32 motions** depuis le train set
2. Pour chaque groupe, choisit **une motion** comme requ√™te (texte)
3. Les 32 motions sont les candidates ‚Äî une seule est la bonne r√©ponse
4. Sauvegarde les paires et les v√©rit√©s terrain dans `gt.csv`

### M√©trique : Recall@K pond√©r√©

```
Score = Œ£ (1/k √ó Recall@k)  /  Œ£ (1/k)     pour k = 1..10
```

| k | Poids | Interpr√©tation |
|---|-------|----------------|
| 1 | 1.000 | La bonne motion est-elle en 1√®re position ? |
| 2 | 0.500 | Est-elle dans le top-2 ? |
| 5 | 0.200 | Est-elle dans le top-5 ? |
| 10| 0.100 | Est-elle dans le top-10 ? |

Bien classer en position 1 est **10√ó plus important** que de la trouver en position 10.

```python
score = eval_recall(gt_df, submission_df, verbose=True)
# k=1 => recall@1=0.45
# k=2 => recall@2=0.61
# ...
# Score pond√©r√© : 0.52
```

---

## Pr√©diction & Soumission

Pour chaque query (texte) face √† ses N motions candidates :

```python
# 1. Encoder le texte
text_emb = encode_text(tokenizer([query_text]))         # (1, 512)

# 2. Encoder toutes les motions candidates
motion_embs = encode_motion(motions)                    # (N, 512)

# 3. Similarit√© cosinus ‚Üí classement
sims    = (text_emb @ motion_embs.T).squeeze(0)         # (N,)
top_idx = torch.topk(sims, k=10).indices                # top-10

# 4. Construire la ligne de soumission
row = {'query_id': id, 'candidate_1': ..., ..., 'candidate_10': ...}
```

Le fichier `submission.csv` a le format :

```
query_id,candidate_1,candidate_2,...,candidate_10
1,15,3,27,8,...
2,4,21,9,12,...
```

---

## Hyperparam√®tres

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| `EMBED_DIM` | 512 | M√™me dimension que CLIP text |
| `MOTION_DIM` | 384 | Dimension d'une frame brute |
| `MAX_SEQ_LEN` | 512 | Couvre ~99% des s√©quences |
| `BATCH_SIZE` | 64 | Plus grand = plus de n√©gatifs InfoNCE |
| `EPOCHS` | 30 | Convergence observ√©e empiriquement |
| `LR` (motion enc) | 1e-4 | Standard AdamW |
| `LR` (CLIP) | 1e-5 | Fine-tuning doux |
| `TEMPERATURE` | 0.07 | Valeur CLIP originale |
| `nhead` | 8 | Standard Transformer |
| `num_layers` | 4 | Compromis capacit√©/vitesse |

---

## R√©sultats & Visualisation

### Courbe de loss

La loss InfoNCE diminue au fil des √©poques sur train et val. Une divergence entre les deux courbes indique de l'overfitting ‚Üí r√©duire les epochs ou augmenter le dropout.

### t-SNE de l'espace latent

Apr√®s entra√Ænement, on encode N paires (texte, motion) et on projette en 2D via t-SNE. Les lignes grises relient chaque texte √† sa motion correspondante.

- **Avant entra√Ænement** : lignes longues et al√©atoires ‚Äî les deux modalit√©s sont dans des r√©gions s√©par√©es de l'espace
- **Apr√®s entra√Ænement** : lignes courtes ‚Äî textes et motions correspondants sont proches

```python
combined = np.vstack([text_arr, motion_arr])   # (2N, 512)
proj_2d  = TSNE(n_components=2).fit_transform(combined)
```

> Le t-SNE est calcul√© sur la **concat√©nation** texte+motion pour que les deux modalit√©s soient dans le m√™me espace de projection 2D, permettant une comparaison visuelle directe.

---

## R√©f√©rences

- [CLIP ‚Äî Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (Radford et al., OpenAI 2021)
- [InfoNCE ‚Äî Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748) (van den Oord et al., DeepMind 2018)
- [TMR ‚Äî Text-to-Motion Retrieval](https://arxiv.org/abs/2305.00976) (Petrovich et al., 2023)
- [HumanML3D Dataset](https://github.com/EricGuo5513/HumanML3D) (Guo et al., 2022)