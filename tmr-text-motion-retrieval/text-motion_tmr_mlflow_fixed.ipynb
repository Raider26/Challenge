{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text‚ÄìMotion Retrieval ‚Äî Version TMR Paper + MLflow\n",
    "\n",
    "**Architectures disponibles:**\n",
    "1. `transformer` ‚Äî Transformer de base (learnable pos encoding, mean pooling)\n",
    "2. `humanml3d` ‚Äî Architecture papier TMR (sinusoidal pos, action token, GELU, 8 layers)\n",
    "\n",
    "**Am√©liorations vs version pr√©c√©dente:**\n",
    "- Deux architectures coexistent proprement, s√©lectionnables via `MOTION_MODEL_TYPE`\n",
    "- Bug fix: validation loss avec `AlignedContrastiveLoss` (retourne tuple)\n",
    "- Bug fix: m√©triques avanc√©es alias (`centroid_dist`, `gap`, `recall@1`)\n",
    "- Bug fix: `create_motion_encoder` if/elif corrig√©\n",
    "- Bug fix: double `forward` supprim√© dans HumanML3DTransformer\n",
    "- Config nettoy√©e (plus de doublons)\n",
    "- `AlignedContrastiveLoss` toujours active en priorit√© si `USE_ALIGNED_LOSS=True`\n",
    "- LSTM toujours disponible comme alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. CONFIGURATION CENTRALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã CONFIGURATION\n",
      "================================================================================\n",
      "Text Encoder  : DISTILBERT\n",
      "Motion Encoder: HUMANML3D\n",
      "Loss          : AlignedContrastive\n",
      "Embed Dim     : 256\n",
      "Layers        : 8\n",
      "Temperature   : 0.05\n",
      "MSE Weight    : 0.15\n",
      "Dropout       : 0.1\n",
      "Batch Size    : 32\n",
      "LR            : 1e-04\n",
      "Duplication   : True\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGURATION - Bas√©e sur le papier TMR\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration bas√©e sur le papier TMR/T-EMOS\"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ CHEMINS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    DATA_ROOT = './data/'\n",
    "    SAVE_DIR = './checkpoints/'\n",
    "    MLFLOW_TRACKING_URI = './mlruns'\n",
    "    MLFLOW_EXPERIMENT_NAME = 'text-motion-retrieval'\n",
    "\n",
    "    # ‚îÄ‚îÄ TEXT ENCODER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Options: 'clip', 'distilbert'\n",
    "    TEXT_ENCODER_TYPE = 'distilbert'\n",
    "    DISTILBERT_MODEL = 'distilbert-base-uncased'\n",
    "    FREEZE_TEXT_ENCODER = False\n",
    "\n",
    "    # ‚îÄ‚îÄ MOTION ENCODER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Options: 'lstm', 'transformer', 'humanml3d'\n",
    "    # 'transformer'  ‚Üí Transformer de base (learnable pos, mean pool)\n",
    "    # 'humanml3d'    ‚Üí Architecture TMR paper (sinusoidal pos, action token, GELU)\n",
    "    MOTION_MODEL_TYPE = 'humanml3d'\n",
    "\n",
    "    # ‚îÄ‚îÄ DIMENSIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    MOTION_DIM = 384\n",
    "    EMBED_DIM = 256\n",
    "    MAX_SEQ_LEN = 196\n",
    "\n",
    "    # ‚îÄ‚îÄ ARCHITECTURE: TRANSFORMER (commun aux deux) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    TRANS_NHEAD = 8\n",
    "    TRANS_NUM_LAYERS = 8        # 4 pour transformer basique, 8 pour humanml3d\n",
    "    TRANS_DIM_FEEDFORWARD = 1024\n",
    "\n",
    "    # ‚îÄ‚îÄ ARCHITECTURE: LSTM (Alternative) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    LSTM_HIDDEN_DIM = 512\n",
    "    LSTM_NUM_LAYERS = 2\n",
    "    LSTM_BIDIRECTIONAL = True\n",
    "\n",
    "    # ‚îÄ‚îÄ LOSS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # USE_ALIGNED_LOSS=True  ‚Üí AlignedContrastiveLoss (contrastive + MSE + uniformity)\n",
    "    # USE_ALIGNED_LOSS=False ‚Üí TEMOSLoss (contrastive + KL)\n",
    "    USE_ALIGNED_LOSS = True\n",
    "    MSE_WEIGHT = 0.15          # Poids MSE sur paires positives (force alignement)\n",
    "    UNIFORMITY_WEIGHT = 0.01   # Poids uniformity (√©vite collapse)\n",
    "\n",
    "    USE_TEMOS_LOSS = True      # Utilis√© si USE_ALIGNED_LOSS=False\n",
    "    LAMBDA_KL = 1e-5\n",
    "    LAMBDA_EXTRA = 0.1\n",
    "\n",
    "    TEMPERATURE = 0.05\n",
    "\n",
    "    # ‚îÄ‚îÄ M√âTRIQUES AVANC√âES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    LOG_SIMILARITY_MATRIX = True\n",
    "    LOG_ADVANCED_METRICS = True\n",
    "\n",
    "    # ‚îÄ‚îÄ R√âGULARISATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    DROPOUT = 0.1\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "    GRAD_CLIP = 0.5\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "\n",
    "    # Augmentation\n",
    "    USE_TIME_MASKING = True\n",
    "    TIME_MASK_PROB = 0.25\n",
    "    USE_NOISE_INJECTION = True\n",
    "    NOISE_STD = 0.01\n",
    "    USE_TEMPORAL_SHIFT = True\n",
    "    TEMPORAL_SHIFT_MAX = 20\n",
    "\n",
    "    # ‚îÄ‚îÄ DUPLICATION DES MOTIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    DUPLICATE_MOTIONS = True\n",
    "\n",
    "    # ‚îÄ‚îÄ ENTRA√éNEMENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    USE_WARMUP = True\n",
    "    WARMUP_EPOCHS = 5\n",
    "    SCHEDULER_TYPE = 'cosine'\n",
    "\n",
    "    PATIENCE = 7\n",
    "    MIN_DELTA = 1e-4\n",
    "\n",
    "    # ‚îÄ‚îÄ VALIDATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    VAL_SPLIT = 0.2\n",
    "    N_VIZ_SAMPLES = 100\n",
    "    VIZ_EVERY_N_EPOCHS = 2\n",
    "\n",
    "    # ‚îÄ‚îÄ MLFLOW ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    LOG_EVERY_N_STEPS = 10\n",
    "    LOG_MODEL = True\n",
    "\n",
    "    # ‚îÄ‚îÄ RANDOM SEED ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã CONFIGURATION\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Text Encoder  : {cls.TEXT_ENCODER_TYPE.upper()}\")\n",
    "        print(f\"Motion Encoder: {cls.MOTION_MODEL_TYPE.upper()}\")\n",
    "        print(f\"Loss          : {'AlignedContrastive' if cls.USE_ALIGNED_LOSS else ('T-EMOS' if cls.USE_TEMOS_LOSS else 'NT-Xent')}\")\n",
    "        print(f\"Embed Dim     : {cls.EMBED_DIM}\")\n",
    "        print(f\"Layers        : {cls.TRANS_NUM_LAYERS}\")\n",
    "        print(f\"Temperature   : {cls.TEMPERATURE}\")\n",
    "        print(f\"MSE Weight    : {cls.MSE_WEIGHT if cls.USE_ALIGNED_LOSS else 'N/A'}\")\n",
    "        print(f\"Dropout       : {cls.DROPOUT}\")\n",
    "        print(f\"Batch Size    : {cls.BATCH_SIZE}\")\n",
    "        print(f\"LR            : {cls.LEARNING_RATE:.0e}\")\n",
    "        print(f\"Duplication   : {cls.DUPLICATE_MOTIONS}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "Config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda\n",
      "‚úì Imports charg√©s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import gc\n",
    "import pickle\n",
    "from os.path import join as pjoin\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "# Text encoders\n",
    "import open_clip\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Info NCE\n",
    "try:\n",
    "    from info_nce import InfoNCE\n",
    "    HAS_INFO_NCE = True\n",
    "except:\n",
    "    HAS_INFO_NCE = False\n",
    "    print(\"‚ö†Ô∏è  info_nce not installed, using custom implementation\")\n",
    "\n",
    "# Setup seeds\n",
    "torch.manual_seed(Config.RANDOM_SEED)\n",
    "np.random.seed(Config.RANDOM_SEED)\n",
    "random.seed(Config.RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "from IPython import display as IPython_display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display as ipy_display\n",
    "import matplotlib\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "%matplotlib inline\n",
    "\n",
    "os.makedirs(Config.SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MLFLOW_TRACKING_URI, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Imports charg√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä MLflow configur√©\n",
      "   Tracking URI: ./mlruns\n",
      "   Experiment: text-motion-retrieval\n",
      "   Run name: distilbert_humanml3d_20260219_155223\n",
      "\n",
      "üí° Pour voir les r√©sultats: mlflow ui --backend-store-uri ./mlruns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luceu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MLFLOW SETUP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(Config.MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "RUN_NAME = f\"{Config.TEXT_ENCODER_TYPE}_{Config.MOTION_MODEL_TYPE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"\\nüìä MLflow configur√©\")\n",
    "print(f\"   Tracking URI: {Config.MLFLOW_TRACKING_URI}\")\n",
    "print(f\"   Experiment: {Config.MLFLOW_EXPERIMENT_NAME}\")\n",
    "print(f\"   Run name: {RUN_NAME}\")\n",
    "print(f\"\\nüí° Pour voir les r√©sultats: mlflow ui --backend-store-uri {Config.MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinematic_chain = [\n",
    "    [0, 2, 5, 8, 11],\n",
    "    [0, 1, 4, 7, 10],\n",
    "    [0, 3, 6, 9, 12, 15],\n",
    "    [9, 14, 17, 19, 21],\n",
    "    [9, 13, 16, 18, 20]\n",
    "]\n",
    "\n",
    "def plot_3d_motion(save_path, kinematic_tree, mp_joints, title, figsize=(10, 10), fps=30, radius=4):\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    title_sp = title.split(' ')\n",
    "    if len(title_sp) > 20:\n",
    "        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:20]), ' '.join(title_sp[20:])])\n",
    "    elif len(title_sp) > 10:\n",
    "        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:])])\n",
    "\n",
    "    def init():\n",
    "        ax.set_xlim3d([-radius / 4, radius / 4])\n",
    "        ax.set_ylim3d([0, radius / 2])\n",
    "        ax.set_zlim3d([0, radius / 2])\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        ax.grid(b=False)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    init()\n",
    "\n",
    "    mp_data = []\n",
    "    frame_number = min([data.shape[0] for data in mp_joints])\n",
    "    colors = ['red', 'green', 'black', 'red', 'blue',\n",
    "              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n",
    "              'darkred', 'darkred', 'darkred', 'darkred', 'darkred']\n",
    "    mp_colors = [[colors[i]] * 15 for i in range(len(mp_joints))]\n",
    "\n",
    "    for joints in mp_joints:\n",
    "        data = joints.copy().reshape(len(joints), -1, 3)\n",
    "        MINS = data.min(axis=0).min(axis=0)\n",
    "        data[:, :, 1] -= MINS[1]\n",
    "        mp_data.append({\"joints\": data})\n",
    "\n",
    "    def update(index):\n",
    "        for line in ax.lines:\n",
    "            line.remove()\n",
    "        for collection in ax.collections:\n",
    "            collection.remove()\n",
    "        ax.view_init(elev=120, azim=-90)\n",
    "        for pid, data in enumerate(mp_data):\n",
    "            for i, (chain, color) in enumerate(zip(kinematic_tree, mp_colors[pid])):\n",
    "                linewidth = 4.0 if i < 5 else 2.0\n",
    "                ax.plot3D(\n",
    "                    data[\"joints\"][index, chain, 0],\n",
    "                    data[\"joints\"][index, chain, 1],\n",
    "                    data[\"joints\"][index, chain, 2],\n",
    "                    linewidth=linewidth, color=color\n",
    "                )\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=frame_number, interval=1000 / fps, repeat=False)\n",
    "    ani.save(save_path, fps=fps)\n",
    "    plt.close()\n",
    "    print(f'Animation sauvegard√©e : {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_and_log_similarity_matrix(text_emb, motion_emb, epoch, save_path=None):\n",
    "    \"\"\"Plot et log matrice de similarit√©\"\"\"\n",
    "    text_emb_np = text_emb.detach().cpu().numpy()\n",
    "    motion_emb_np = motion_emb.detach().cpu().numpy()\n",
    "\n",
    "    sim_matrix = text_emb_np @ motion_emb_np.T\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        sim_matrix, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "        square=True, ax=axes[0], cbar_kws={'label': 'Cosine Similarity'},\n",
    "        xticklabels=False, yticklabels=False\n",
    "    )\n",
    "    axes[0].set_title(f'Similarity Matrix (Epoch {epoch})', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Motion Index')\n",
    "    axes[0].set_ylabel('Text Index')\n",
    "\n",
    "    mask = np.ones_like(sim_matrix, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    sns.heatmap(\n",
    "        sim_matrix, mask=mask, cmap='Greens', vmin=0, vmax=1,\n",
    "        square=True, ax=axes[1], cbar_kws={'label': 'Similarity'},\n",
    "        xticklabels=False, yticklabels=False\n",
    "    )\n",
    "    axes[1].set_title('Correct Pairs (Diagonal)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Motion Index')\n",
    "    axes[1].set_ylabel('Text Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = f'similarity_matrix_epoch_{epoch}.png'\n",
    "\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "    try:\n",
    "        mlflow.log_artifact(save_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'diagonal_sim': np.mean(np.diag(sim_matrix)),\n",
    "        'off_diagonal_sim': np.mean(sim_matrix[~np.eye(len(sim_matrix), dtype=bool)]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì M√©triques avanc√©es d√©finies\n"
     ]
    }
   ],
   "source": [
    "def compute_advanced_metrics(text_emb, motion_emb):\n",
    "    \"\"\"M√©triques avanc√©es pour diagnostic du probl√®me de s√©paration modale\"\"\"\n",
    "    text_np = text_emb.detach().cpu().numpy()\n",
    "    motion_np = motion_emb.detach().cpu().numpy()\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Intra-modalit√©\n",
    "    text_sim = text_np @ text_np.T\n",
    "    motion_sim = motion_np @ motion_np.T\n",
    "    mask = ~np.eye(len(text_np), dtype=bool)\n",
    "    metrics['intra_text_sim'] = float(text_sim[mask].mean())\n",
    "    metrics['intra_motion_sim'] = float(motion_sim[mask].mean())\n",
    "\n",
    "    # Inter-modalit√©\n",
    "    cross_sim = text_np @ motion_np.T\n",
    "    metrics['inter_modal_mean'] = float(cross_sim.mean())\n",
    "    metrics['inter_modal_std'] = float(cross_sim.std())\n",
    "\n",
    "    # Distance centro√Ødes (cible: < 0.3 ‚Üí pas de s√©paration modale)\n",
    "    text_centroid = text_np.mean(axis=0)\n",
    "    motion_centroid = motion_np.mean(axis=0)\n",
    "    metrics['centroid_dist'] = float(np.linalg.norm(text_centroid - motion_centroid))\n",
    "\n",
    "    # Recall@K\n",
    "    for k in [1, 5, 10]:\n",
    "        correct = sum(\n",
    "            i in np.argsort(cross_sim[i])[::-1][:k]\n",
    "            for i in range(len(text_np))\n",
    "        )\n",
    "        metrics[f'recall_at_{k}'] = correct / len(text_np)\n",
    "\n",
    "    # Diagonal vs off-diagonal\n",
    "    diagonal = float(np.diag(cross_sim).mean())\n",
    "    off_diag = float(cross_sim[mask].mean())\n",
    "    metrics['diagonal_sim'] = diagonal\n",
    "    metrics['off_diagonal_sim'] = off_diag\n",
    "    metrics['gap'] = diagonal - off_diag  # cible: > 0.7\n",
    "\n",
    "    # Alias courts pour affichage\n",
    "    metrics['recall@1'] = metrics['recall_at_1']\n",
    "    metrics['recall@5'] = metrics['recall_at_5']\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì M√©triques avanc√©es d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonction d'√©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall(gt_df, pred_df, verbose=False):\n",
    "    \"\"\"Calcule le score Recall@K pond√©r√© (K = 1..10).\"\"\"\n",
    "    ks = list(range(1, 11))\n",
    "    rank_cols = list(pred_df.columns)\n",
    "    gt = gt_df[\"candidate\"].values\n",
    "    preds = pred_df[rank_cols].values\n",
    "    assert len(gt) == len(preds), \"Taille gt / preds incompatible\"\n",
    "\n",
    "    recalls = {k: 0 for k in ks}\n",
    "    n = len(gt)\n",
    "    for i in range(n):\n",
    "        for k in ks:\n",
    "            if gt[i] in preds[i, :k]:\n",
    "                recalls[k] += 1\n",
    "    for k in ks:\n",
    "        recalls[k] /= n\n",
    "\n",
    "    weights = {k: 1.0 / k for k in ks}\n",
    "    weight_sum = sum(weights.values())\n",
    "    final_score = sum(weights[k] * recalls[k] for k in ks) / weight_sum\n",
    "\n",
    "    if verbose:\n",
    "        for k, val in recalls.items():\n",
    "            print(f'k={k} => recall@{k}={round(val, 3)}')\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. G√©n√©ration val batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_batches(data_root, save_dir, batch_size, num_batches):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(os.path.join(data_root, 'train.txt')) as fd:\n",
    "        train_fnames = fd.read().strip().split('\\n')\n",
    "\n",
    "    random_fnames = random.sample(train_fnames, k=batch_size * num_batches)\n",
    "    random_batches = np.array(random_fnames).reshape(num_batches, batch_size)\n",
    "\n",
    "    gt = [['query_id', 'candidate']]\n",
    "    for file_idx, batch in enumerate(random_batches, start=1):\n",
    "        os.makedirs(pjoin(save_dir, str(file_idx)), exist_ok=True)\n",
    "        rdm_idx = random.randint(0, len(batch) - 1)\n",
    "        random_text_fname = batch[rdm_idx]\n",
    "\n",
    "        with open(pjoin(data_root, 'texts', random_text_fname + '.txt')) as fd:\n",
    "            file_texts = fd.read().strip().split('\\n')\n",
    "            random_text = random.choice(file_texts)\n",
    "\n",
    "        with open(pjoin(save_dir, str(file_idx), 'text.txt'), 'w') as fw:\n",
    "            fw.write(random_text)\n",
    "\n",
    "        for motion_idx, fname in enumerate(batch, start=1):\n",
    "            if fname == random_text_fname:\n",
    "                gt.append([file_idx, motion_idx])\n",
    "            motion = np.load(pjoin(data_root, 'motions', fname + '.npy'))\n",
    "            np.save(pjoin(save_dir, str(file_idx), f'motion_{motion_idx}.npy'), motion)\n",
    "\n",
    "    with open(pjoin(save_dir, 'gt.csv'), 'w', newline='') as f:\n",
    "        csv.writer(f).writerows(gt)\n",
    "    print(f'Val batches sauvegard√©s dans {save_dir}')\n",
    "\n",
    "\n",
    "data_root = Config.DATA_ROOT\n",
    "save_dir = pjoin(Config.DATA_ROOT, 'val/')\n",
    "if not os.path.exists(pjoin(save_dir, 'gt.csv')):\n",
    "    generate_val_batches(data_root, save_dir, batch_size=32, num_batches=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TEXT ENCODERS (CLIP + DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Cr√©ation text encoder: DISTILBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de961762cb304552bf118d440b8b9fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DistilBERT charg√© | Freeze: False\n",
      "   Param√®tres: 66,888,960 (entra√Ænables: 66,888,960)\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TEXT ENCODERS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class DistilBertTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text encoder bas√© sur DistilBERT (comme dans le papier TMR).\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=256, freeze=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(Config.DISTILBERT_MODEL)\n",
    "        self.bert_dim = self.bert.config.hidden_size  # 768\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert_dim, output_dim * 2),\n",
    "            nn.LayerNorm(output_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(output_dim * 2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.projection(cls_output)\n",
    "\n",
    "\n",
    "def create_text_encoder():\n",
    "    \"\"\"\n",
    "    Cr√©e le text encoder selon Config.\n",
    "    Retourne (encoder, tokenizer, tokenize_function)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Cr√©ation text encoder: {Config.TEXT_ENCODER_TYPE.upper()}\")\n",
    "\n",
    "    if Config.TEXT_ENCODER_TYPE == 'distilbert':\n",
    "        encoder = DistilBertTextEncoder(\n",
    "            output_dim=Config.EMBED_DIM,\n",
    "            freeze=Config.FREEZE_TEXT_ENCODER,\n",
    "            dropout=Config.DROPOUT\n",
    "        ).to(device)\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(Config.DISTILBERT_MODEL)\n",
    "\n",
    "        def tokenize_fn(texts):\n",
    "            return tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        print(f\"   DistilBERT charg√© | Freeze: {Config.FREEZE_TEXT_ENCODER}\")\n",
    "\n",
    "    elif Config.TEXT_ENCODER_TYPE == 'clip':\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "            'ViT-B-32',\n",
    "            pretrained='laion2b_s34b_b79k'\n",
    "        )\n",
    "\n",
    "        class CLIPTextWrapper(nn.Module):\n",
    "            def __init__(self, clip_model, output_dim):\n",
    "                super().__init__()\n",
    "                self.clip = clip_model\n",
    "                self.projection = nn.Linear(512, output_dim) if output_dim != 512 else nn.Identity()\n",
    "\n",
    "            def forward(self, input_ids, attention_mask=None):\n",
    "                feats = self.clip.encode_text(input_ids)\n",
    "                return self.projection(feats.float())\n",
    "\n",
    "        encoder = CLIPTextWrapper(clip_model, Config.EMBED_DIM).to(device)\n",
    "\n",
    "        if Config.FREEZE_TEXT_ENCODER:\n",
    "            for param in encoder.clip.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "        def tokenize_fn(texts):\n",
    "            tokens = tokenizer(texts)\n",
    "            return {'input_ids': tokens, 'attention_mask': None}\n",
    "\n",
    "        print(f\"   CLIP charg√©\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown text encoder: {Config.TEXT_ENCODER_TYPE}\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in encoder.parameters())\n",
    "    trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "    print(f\"   Param√®tres: {total_params:,} (entra√Ænables: {trainable_params:,})\")\n",
    "\n",
    "    return encoder, tokenizer, tokenize_fn\n",
    "\n",
    "\n",
    "text_encoder, tokenizer, tokenize_texts = create_text_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MOTION ENCODERS\n",
    "\n",
    "**Deux architectures disponibles via `Config.MOTION_MODEL_TYPE`:**\n",
    "- `'transformer'` ‚Äî Transformer de base (learnable positional encoding, mean pooling)\n",
    "- `'humanml3d'` ‚Äî Architecture du papier TMR (sinusoidal pos, action token CLS-like, GELU, 8 layers)\n",
    "- `'lstm'` ‚Äî LSTM Bidirectionnel avec Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  Cr√©ation motion encoder: HUMANML3D\n",
      "   Sinusoidal pos encoding | Action token (CLS) | GELU\n",
      "   Param√®tres: 6,483,712\n",
      "\n",
      "‚úì Encodeurs cr√©√©s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luceu\\AppData\\Local\\Temp\\ipykernel_35380\\1504364071.py:96: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  self.transformer = nn.TransformerEncoder(\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MOTION ENCODERS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class TransformerMotionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Motion Encoder de base.\n",
    "    - Positional encoding LEARNABLE\n",
    "    - Mean pooling sur la s√©quence\n",
    "    - Activation ReLU\n",
    "    \n",
    "    Utiliser quand: MOTION_MODEL_TYPE = 'transformer'\n",
    "    \"\"\"\n",
    "    def __init__(self, motion_dim, embed_dim, nhead=8, num_layers=4,\n",
    "                 dim_feedforward=1024, dropout=0.1, max_seq_len=196):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(motion_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "\n",
    "        # Positional encoding LEARNABLE\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, embed_dim) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_embed[:, :T, :]\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Mean pooling (ignorer le padding)\n",
    "        if padding_mask is not None:\n",
    "            mask = (~padding_mask).float().unsqueeze(-1)\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HumanML3DTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture du papier TMR/HumanML3D.\n",
    "    Diff√©rences vs TransformerMotionEncoder:\n",
    "    - Positional encoding SINUSOIDAL (plus stable, pas de param√®tres)\n",
    "    - Action token (comme [CLS] dans BERT) ‚Üí pas besoin de pooling\n",
    "    - Activation GELU (plus smooth que ReLU)\n",
    "    - 8 layers par d√©faut\n",
    "\n",
    "    Utiliser quand: MOTION_MODEL_TYPE = 'humanml3d'\n",
    "    \"\"\"\n",
    "    def __init__(self, motion_dim=384, embed_dim=256, num_layers=8, nhead=8,\n",
    "                 dim_feedforward=1024, dropout=0.1, max_seq_len=196):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Input projection (pas de LayerNorm ici, la norme vient du transformer)\n",
    "        self.input_proj = nn.Linear(motion_dim, embed_dim)\n",
    "\n",
    "        # Positional encoding SINUSOIDAL (buffer, non-entra√Ænable)\n",
    "        self.register_buffer(\n",
    "            'pos_encoding',\n",
    "            self._create_sinusoidal_encoding(max_seq_len, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Transformer avec GELU\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "\n",
    "        # Action token (comme [CLS] dans BERT)\n",
    "        self.action_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def _create_sinusoidal_encoding(self, max_len, d_model):\n",
    "        \"\"\"Positional encoding sinusoidal (Attention is All You Need)\"\"\"\n",
    "        position = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Input projection + positional encoding sinusoidal\n",
    "        x = self.input_proj(x) + self.pos_encoding[:, :T, :]\n",
    "\n",
    "        # Prepend action token (CLS-like)\n",
    "        action_tokens = self.action_token.expand(B, -1, -1)\n",
    "        x = torch.cat([action_tokens, x], dim=1)  # (B, T+1, D)\n",
    "\n",
    "        # Mettre √† jour le padding mask pour inclure l'action token\n",
    "        if padding_mask is not None:\n",
    "            action_mask = torch.zeros(B, 1, dtype=torch.bool, device=x.device)\n",
    "            padding_mask = torch.cat([action_mask, padding_mask], dim=1)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Utiliser l'action token comme repr√©sentation finale (position 0)\n",
    "        return self.output_proj(x[:, 0, :])\n",
    "\n",
    "\n",
    "class LSTMMotionEncoder(nn.Module):\n",
    "    \"\"\"LSTM Bidirectionnel avec Attention.\"\"\"\n",
    "    def __init__(self, motion_dim, hidden_dim, embed_dim, num_layers=2,\n",
    "                 dropout=0.1, bidirectional=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(motion_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        lstm_hidden = (hidden_dim // 2) if bidirectional else hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim // 2,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        lstm_out_dim = hidden_dim if bidirectional else lstm_hidden\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, lstm_out_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(lstm_out_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_out_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_out_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        x = self.input_proj(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        attn_scores = self.attention(lstm_out)\n",
    "        if padding_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(padding_mask.unsqueeze(-1), -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        return self.output_proj(context)\n",
    "\n",
    "\n",
    "def create_motion_encoder():\n",
    "    \"\"\"Factory pour cr√©er le motion encoder selon Config.MOTION_MODEL_TYPE.\"\"\"\n",
    "    model_type = Config.MOTION_MODEL_TYPE.lower()\n",
    "\n",
    "    print(f\"\\nüèóÔ∏è  Cr√©ation motion encoder: {model_type.upper()}\")\n",
    "\n",
    "    if model_type == 'transformer':\n",
    "        encoder = TransformerMotionEncoder(\n",
    "            motion_dim=Config.MOTION_DIM,\n",
    "            embed_dim=Config.EMBED_DIM,\n",
    "            nhead=Config.TRANS_NHEAD,\n",
    "            num_layers=Config.TRANS_NUM_LAYERS,\n",
    "            dim_feedforward=Config.TRANS_DIM_FEEDFORWARD,\n",
    "            dropout=Config.DROPOUT,\n",
    "            max_seq_len=Config.MAX_SEQ_LEN\n",
    "        )\n",
    "        print(\"   Learnable pos encoding | Mean pooling | ReLU\")\n",
    "\n",
    "    elif model_type == 'humanml3d':\n",
    "        encoder = HumanML3DTransformer(\n",
    "            motion_dim=Config.MOTION_DIM,\n",
    "            embed_dim=Config.EMBED_DIM,\n",
    "            num_layers=Config.TRANS_NUM_LAYERS,\n",
    "            nhead=Config.TRANS_NHEAD,\n",
    "            dim_feedforward=Config.TRANS_DIM_FEEDFORWARD,\n",
    "            dropout=Config.DROPOUT,\n",
    "            max_seq_len=Config.MAX_SEQ_LEN\n",
    "        )\n",
    "        print(\"   Sinusoidal pos encoding | Action token (CLS) | GELU\")\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        encoder = LSTMMotionEncoder(\n",
    "            motion_dim=Config.MOTION_DIM,\n",
    "            hidden_dim=Config.LSTM_HIDDEN_DIM,\n",
    "            embed_dim=Config.EMBED_DIM,\n",
    "            num_layers=Config.LSTM_NUM_LAYERS,\n",
    "            dropout=Config.DROPOUT,\n",
    "            bidirectional=Config.LSTM_BIDIRECTIONAL\n",
    "        )\n",
    "        print(\"   Bidirectional LSTM | Attention pooling\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: '{model_type}'. Use 'transformer', 'humanml3d', or 'lstm'.\")\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    total_params = sum(p.numel() for p in encoder.parameters())\n",
    "    print(f\"   Param√®tres: {total_params:,}\")\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "# is_vae_model est toujours False (VAE non support√© dans cette version)\n",
    "is_vae_model = False\n",
    "motion_encoder = create_motion_encoder()\n",
    "print(\"\\n‚úì Encodeurs cr√©√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset avec augmentation renforc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset et preprocessing d√©finis\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PREPROCESSING avec AUGMENTATION RENFORC√âE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def compute_global_stats(fnames, motion_dir, n_samples=500):\n",
    "    sample = random.sample(fnames, min(n_samples, len(fnames)))\n",
    "    all_frames = [np.load(os.path.join(motion_dir, f + '.npy')) for f in sample]\n",
    "    all_frames = np.vstack(all_frames)\n",
    "    return all_frames.mean(axis=0), all_frames.std(axis=0) + 1e-6\n",
    "\n",
    "\n",
    "def preprocess_motion(motion, global_mean=None, global_std=None, max_len=None, augment=False):\n",
    "    \"\"\"\n",
    "    Pr√©traite avec augmentation renforc√©e.\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = Config.MAX_SEQ_LEN\n",
    "\n",
    "    if global_mean is not None and global_std is not None:\n",
    "        motion = (motion - global_mean) / global_std\n",
    "\n",
    "    if augment:\n",
    "        # 1. Time masking\n",
    "        if Config.USE_TIME_MASKING and Config.TIME_MASK_PROB > 0:\n",
    "            mask = np.random.random(len(motion)) < Config.TIME_MASK_PROB\n",
    "            motion[mask] = 0\n",
    "\n",
    "        # 2. Noise injection\n",
    "        if Config.USE_NOISE_INJECTION and Config.NOISE_STD > 0:\n",
    "            noise = np.random.normal(0, Config.NOISE_STD, motion.shape)\n",
    "            motion = motion + noise\n",
    "\n",
    "        # 3. Temporal shift\n",
    "        if Config.USE_TEMPORAL_SHIFT and Config.TEMPORAL_SHIFT_MAX > 0:\n",
    "            shift = np.random.randint(-Config.TEMPORAL_SHIFT_MAX, Config.TEMPORAL_SHIFT_MAX + 1)\n",
    "            if shift != 0:\n",
    "                motion = np.roll(motion, shift, axis=0)\n",
    "\n",
    "    T, F = motion.shape\n",
    "    mask = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    if T >= max_len:\n",
    "        if augment and T > max_len:\n",
    "            start_idx = np.random.randint(0, T - max_len + 1)\n",
    "            motion = motion[start_idx:start_idx + max_len]\n",
    "        else:\n",
    "            motion = motion[:max_len]\n",
    "    else:\n",
    "        mask[T:] = True\n",
    "        motion = np.vstack([motion, np.zeros((max_len - T, F), dtype=np.float32)])\n",
    "\n",
    "    return torch.from_numpy(motion).float(), torch.from_numpy(mask)\n",
    "\n",
    "\n",
    "class TextMotionDataset(Dataset):\n",
    "    def __init__(self, fnames, data_root, global_mean, global_std,\n",
    "                 max_seq_len=None, duplicate_motions=None, augment=False):\n",
    "        self.motion_dir = pjoin(data_root, 'motions')\n",
    "        self.text_dir = pjoin(data_root, 'texts')\n",
    "        self.max_seq_len = max_seq_len or Config.MAX_SEQ_LEN\n",
    "        self.global_mean = global_mean\n",
    "        self.global_std = global_std\n",
    "        self.augment = augment\n",
    "        self.duplicate_motions = duplicate_motions if duplicate_motions is not None else Config.DUPLICATE_MOTIONS\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        for fname in fnames:\n",
    "            text_path = pjoin(self.text_dir, fname + '.txt')\n",
    "            if os.path.exists(text_path):\n",
    "                with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                    texts = [l.strip() for l in f.readlines() if l.strip()]\n",
    "\n",
    "                if self.duplicate_motions:\n",
    "                    for t in texts:\n",
    "                        self.samples.append((fname, t))\n",
    "                else:\n",
    "                    self.samples.append((fname, texts))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname, text_data = self.samples[idx]\n",
    "\n",
    "        if self.duplicate_motions:\n",
    "            text = text_data\n",
    "        else:\n",
    "            text = random.choice(text_data)\n",
    "\n",
    "        motion_raw = np.load(pjoin(self.motion_dir, fname + '.npy'))\n",
    "        motion, pad_mask = preprocess_motion(\n",
    "            motion_raw, self.global_mean, self.global_std,\n",
    "            max_len=self.max_seq_len, augment=self.augment\n",
    "        )\n",
    "\n",
    "        return text, motion, pad_mask\n",
    "\n",
    "\n",
    "print(\"‚úì Dataset et preprocessing d√©finis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fichiers train : 6018\n",
      "Stats globales calcul√©es. Mean shape: (384,)\n",
      "Train : 4814 | Val : 1204\n",
      "Batches train : 451 | Batches val : 38\n",
      "Samples train : 14442 | Samples val : 1204\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es\n",
    "data_root = Config.DATA_ROOT\n",
    "with open(pjoin(data_root, 'train.txt')) as f:\n",
    "    all_fnames = f.read().strip().split('\\n')\n",
    "print(f'Total fichiers train : {len(all_fnames)}')\n",
    "\n",
    "motion_dir_path = pjoin(data_root, 'motions')\n",
    "GLOBAL_MEAN, GLOBAL_STD = compute_global_stats(all_fnames, motion_dir_path)\n",
    "print(f'Stats globales calcul√©es. Mean shape: {GLOBAL_MEAN.shape}')\n",
    "\n",
    "train_fnames, val_fnames = train_test_split(\n",
    "    all_fnames, test_size=Config.VAL_SPLIT, random_state=Config.RANDOM_SEED\n",
    ")\n",
    "print(f'Train : {len(train_fnames)} | Val : {len(val_fnames)}')\n",
    "\n",
    "train_dataset = TextMotionDataset(\n",
    "    train_fnames, data_root, GLOBAL_MEAN, GLOBAL_STD,\n",
    "    duplicate_motions=Config.DUPLICATE_MOTIONS,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = TextMotionDataset(\n",
    "    val_fnames, data_root, GLOBAL_MEAN, GLOBAL_STD,\n",
    "    duplicate_motions=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, motions, masks = zip(*batch)\n",
    "    text_batch = tokenize_texts(list(texts))\n",
    "    text_batch = {k: v.to(device) if v is not None else None for k, v in text_batch.items()}\n",
    "    motion_batch = torch.stack(motions).to(device)\n",
    "    mask_batch = torch.stack(masks).to(device)\n",
    "    return text_batch, motion_batch, mask_batch\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "    drop_last=True, collate_fn=collate_fn, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
    "    drop_last=False, collate_fn=collate_fn, num_workers=0\n",
    ")\n",
    "\n",
    "print(f'Batches train : {len(train_loader)} | Batches val : {len(val_loader)}')\n",
    "print(f'Samples train : {len(train_dataset)} | Samples val : {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LOSS FUNCTIONS\n",
    "\n",
    "**Deux losses disponibles:**\n",
    "- `AlignedContrastiveLoss` (si `USE_ALIGNED_LOSS=True`) : contrastive + MSE paires + uniformity ‚Üí r√©sout la s√©paration modale\n",
    "- `TEMOSLoss` (si `USE_ALIGNED_LOSS=False`) : contrastive + KL (papier)\n",
    "\n",
    "**Pourquoi AlignedContrastiveLoss r√©sout la s√©paration t-SNE:**\n",
    "La MSE loss force chaque paire (text_i, motion_i) √† avoir des embeddings proches dans l'espace L2, en plus de la loss contrastive qui ne fait que s√©parer les paires incorrectes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì AlignedContrastiveLoss (T=0.05, MSE_W=0.15, U_W=0.01)\n",
      "   LOSS_RETURNS_TUPLE = True\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LOSS FUNCTIONS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    \"\"\"NT-Xent (Normalized Temperature-scaled Cross Entropy).\"\"\"\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, text_emb, motion_emb):\n",
    "        batch_size = text_emb.shape[0]\n",
    "        logits = (text_emb @ motion_emb.T) / self.temperature\n",
    "        labels = torch.arange(batch_size, device=text_emb.device)\n",
    "        loss_t2m = F.cross_entropy(logits, labels)\n",
    "        loss_m2t = F.cross_entropy(logits.T, labels)\n",
    "        return (loss_t2m + loss_m2t) / 2\n",
    "\n",
    "\n",
    "class TEMOSLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss T-EMOS du papier: L = L_R + Œª_KL * L_KL + Œª_E * L_E\n",
    "    Retourne (total_loss, loss_dict) pour compatibilit√©.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.05, lambda_kl=1e-5, lambda_extra=0.1):\n",
    "        super().__init__()\n",
    "        self.contrastive_loss = NTXentLoss(temperature)\n",
    "        self.lambda_kl = lambda_kl\n",
    "        self.lambda_extra = lambda_extra\n",
    "\n",
    "    def forward(self, text_emb, motion_emb, mu=None, logvar=None, extra_loss=None):\n",
    "        loss_r = self.contrastive_loss(text_emb, motion_emb)\n",
    "        loss_dict = {'contrastive': loss_r.item()}\n",
    "        total_loss = loss_r\n",
    "\n",
    "        if mu is not None and logvar is not None:\n",
    "            loss_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "            loss_dict['kl'] = loss_kl.item()\n",
    "            total_loss = total_loss + self.lambda_kl * loss_kl\n",
    "\n",
    "        if extra_loss is not None:\n",
    "            loss_dict['extra'] = extra_loss.item()\n",
    "            total_loss = total_loss + self.lambda_extra * extra_loss\n",
    "\n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        return total_loss, loss_dict\n",
    "\n",
    "\n",
    "class AlignedContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss avec alignement forc√© pour r√©soudre la s√©paration modale.\n",
    "    Combine:\n",
    "    1. Contrastive loss (CLIP-style) ‚Äî s√©pare paires incorrectes\n",
    "    2. MSE loss sur paires positives ‚Äî FORCE l'alignement exact\n",
    "    3. Uniformity loss ‚Äî distribution uniforme sur la sph√®re\n",
    "\n",
    "    Retourne (total_loss, loss_dict) pour compatibilit√© avec le training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.05, mse_weight=0.15, uniformity_weight=0.01):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.mse_weight = mse_weight\n",
    "        self.uniformity_weight = uniformity_weight\n",
    "\n",
    "    def forward(self, text_emb, motion_emb):\n",
    "        # Normalisation L2 (embeddings sur la sph√®re unitaire)\n",
    "        text_norm = F.normalize(text_emb, dim=-1)\n",
    "        motion_norm = F.normalize(motion_emb, dim=-1)\n",
    "\n",
    "        batch_size = text_norm.shape[0]\n",
    "\n",
    "        # 1. Contrastive loss bidirectionnelle\n",
    "        logits = (text_norm @ motion_norm.T) / self.temperature\n",
    "        labels = torch.arange(batch_size, device=text_norm.device)\n",
    "        loss_t2m = F.cross_entropy(logits, labels)\n",
    "        loss_m2t = F.cross_entropy(logits.T, labels)\n",
    "        contrastive_loss = (loss_t2m + loss_m2t) / 2\n",
    "\n",
    "        # 2. MSE sur paires positives (force alignement)\n",
    "        mse_loss = F.mse_loss(text_norm, motion_norm)\n",
    "\n",
    "        # 3. Uniformity loss\n",
    "        def _uniformity(z):\n",
    "            if len(z) < 2:\n",
    "                return torch.tensor(0.0, device=z.device)\n",
    "            dists = torch.pdist(z, p=2)\n",
    "            return torch.log(torch.exp(-2 * dists).mean() + 1e-8)\n",
    "\n",
    "        uniformity = (_uniformity(text_norm) + _uniformity(motion_norm)) / 2\n",
    "\n",
    "        total_loss = (\n",
    "            contrastive_loss +\n",
    "            self.mse_weight * mse_loss +\n",
    "            self.uniformity_weight * uniformity\n",
    "        )\n",
    "\n",
    "        loss_dict = {\n",
    "            'contrastive': contrastive_loss.item(),\n",
    "            'mse': mse_loss.item(),\n",
    "            'uniformity': uniformity.item(),\n",
    "            'total': total_loss.item()\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Cr√©ation de la loss selon Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if Config.USE_ALIGNED_LOSS:\n",
    "    loss_fn = AlignedContrastiveLoss(\n",
    "        temperature=Config.TEMPERATURE,\n",
    "        mse_weight=Config.MSE_WEIGHT,\n",
    "        uniformity_weight=Config.UNIFORMITY_WEIGHT\n",
    "    )\n",
    "    print(f\"‚úì AlignedContrastiveLoss (T={Config.TEMPERATURE}, MSE_W={Config.MSE_WEIGHT}, U_W={Config.UNIFORMITY_WEIGHT})\")\n",
    "    LOSS_RETURNS_TUPLE = True\n",
    "\n",
    "elif Config.USE_TEMOS_LOSS:\n",
    "    loss_fn = TEMOSLoss(\n",
    "        temperature=Config.TEMPERATURE,\n",
    "        lambda_kl=Config.LAMBDA_KL,\n",
    "        lambda_extra=Config.LAMBDA_EXTRA\n",
    "    )\n",
    "    print(f\"‚úì TEMOSLoss (T={Config.TEMPERATURE}, Œª_KL={Config.LAMBDA_KL:.0e})\")\n",
    "    LOSS_RETURNS_TUPLE = True\n",
    "\n",
    "else:\n",
    "    loss_fn = NTXentLoss(temperature=Config.TEMPERATURE)\n",
    "    print(f\"‚úì NTXentLoss (T={Config.TEMPERATURE})\")\n",
    "    LOSS_RETURNS_TUPLE = False\n",
    "\n",
    "print(f\"   LOSS_RETURNS_TUPLE = {LOSS_RETURNS_TUPLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fonctions d'encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonctions d'encodage d√©finies\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text_batch):\n",
    "    \"\"\"\n",
    "    Encode le texte et normalise L2.\n",
    "    text_batch: dict avec 'input_ids' et 'attention_mask'\n",
    "    \"\"\"\n",
    "    text_feats = text_encoder(**text_batch)\n",
    "    return F.normalize(text_feats, dim=-1)\n",
    "\n",
    "\n",
    "def encode_motion(motion_batch, mask_batch=None):\n",
    "    \"\"\"\n",
    "    Encode la motion et normalise L2.\n",
    "    Toujours retourne un seul tensor (pas de VAE dans cette version).\n",
    "    \"\"\"\n",
    "    output = motion_encoder(motion_batch, mask_batch)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    return F.normalize(output, dim=-1)\n",
    "\n",
    "\n",
    "def compute_loss(text_emb, motion_emb):\n",
    "    \"\"\"\n",
    "    Calcule la loss et retourne (total_loss, loss_dict) uniform√©ment.\n",
    "    G√®re les deux cas: loss retournant un tuple ou un scalaire.\n",
    "    \"\"\"\n",
    "    if LOSS_RETURNS_TUPLE:\n",
    "        loss, loss_dict = loss_fn(text_emb, motion_emb)\n",
    "    else:\n",
    "        loss = loss_fn(text_emb, motion_emb)\n",
    "        loss_dict = {'total': loss.item(), 'contrastive': loss.item()}\n",
    "    return loss, loss_dict\n",
    "\n",
    "\n",
    "print(\"‚úì Fonctions d'encodage d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Optimiseur avec Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Scheduler: Warmup (5 epochs) + Cosine decay\n",
      "   LR: 1e-04, WD: 1e-03\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# OPTIMISEUR ET SCHEDULER avec WARMUP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "params_to_optimize = list(motion_encoder.parameters()) + list(text_encoder.parameters())\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=Config.LEARNING_RATE,\n",
    "    weight_decay=Config.WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "if Config.USE_WARMUP:\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < Config.WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / Config.WARMUP_EPOCHS\n",
    "        else:\n",
    "            progress = (epoch - Config.WARMUP_EPOCHS) / max(1, Config.EPOCHS - Config.WARMUP_EPOCHS)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(f\"\\n‚úì Scheduler: Warmup ({Config.WARMUP_EPOCHS} epochs) + Cosine decay\")\n",
    "else:\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=Config.EPOCHS, eta_min=Config.LEARNING_RATE / 100\n",
    "    )\n",
    "    print(f\"\\n‚úì Scheduler: Cosine Annealing\")\n",
    "\n",
    "print(f\"   LR: {Config.LEARNING_RATE:.0e}, WD: {Config.WEIGHT_DECAY:.0e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Dashboard fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dashboard ready\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# DASHBOARD LIVE\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def get_live_embeddings(fnames_subset):\n",
    "    text_encoder.eval()\n",
    "    motion_encoder.eval()\n",
    "    t_embs, m_embs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fname in fnames_subset:\n",
    "            with open(pjoin(data_root, 'texts', fname + '.txt')) as f:\n",
    "                txts = f.read().strip().split('\\n')\n",
    "\n",
    "            text_batch = tokenize_texts([txts[0]])\n",
    "            text_batch = {k: v.to(device) if v is not None else None for k, v in text_batch.items()}\n",
    "            t_emb = encode_text(text_batch)\n",
    "\n",
    "            motion_raw = np.load(pjoin(data_root, 'motions', fname + '.npy'))\n",
    "            mot, msk = preprocess_motion(motion_raw, GLOBAL_MEAN, GLOBAL_STD)\n",
    "            mot = mot.unsqueeze(0).to(device)\n",
    "            msk = msk.unsqueeze(0).to(device)\n",
    "            m_emb = encode_motion(mot, msk)\n",
    "\n",
    "            t_embs.append(t_emb.cpu().numpy().squeeze())\n",
    "            m_embs.append(m_emb.cpu().numpy().squeeze())\n",
    "\n",
    "    return np.array(t_embs), np.array(m_embs), fnames_subset\n",
    "\n",
    "\n",
    "def compute_cosine_stats(text_arr, motion_arr):\n",
    "    sim_matrix = text_arr @ motion_arr.T\n",
    "    N = len(text_arr)\n",
    "    diag_sims = sim_matrix[np.arange(N), np.arange(N)]\n",
    "    mask_off = ~np.eye(N, dtype=bool)\n",
    "    offdiag_sims = sim_matrix[mask_off]\n",
    "    return float(diag_sims.mean()), float(offdiag_sims.mean())\n",
    "\n",
    "\n",
    "def render_dashboard(train_losses, val_losses, cos_correct_hist, cos_wrong_hist,\n",
    "                     text_arr, motion_arr, epoch, best_epoch):\n",
    "    combined = np.vstack([text_arr, motion_arr])\n",
    "    tsne = TSNE(n_components=2, perplexity=min(15, len(text_arr) // 2),\n",
    "                random_state=42, verbose=0)\n",
    "    proj = tsne.fit_transform(combined)\n",
    "    N = len(text_arr)\n",
    "    t2d, m2d = proj[:N], proj[N:]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    axes[0].plot(train_losses, label='Train', linewidth=2)\n",
    "    axes[0].plot(val_losses, label='Val', linewidth=2)\n",
    "    axes[0].axvline(best_epoch, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'Loss (Epoch {epoch})', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    epochs_hist = list(range(1, len(cos_correct_hist) + 1))\n",
    "    axes[1].plot(epochs_hist, cos_correct_hist, label='Correct', linewidth=2, color='green')\n",
    "    axes[1].plot(epochs_hist, cos_wrong_hist, label='Incorrect', linewidth=2, color='red')\n",
    "    gap = [c - w for c, w in zip(cos_correct_hist, cos_wrong_hist)]\n",
    "    axes[1].plot(epochs_hist, gap, label='Gap', linestyle='--', linewidth=2, color='blue')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Cosine Similarity')\n",
    "    axes[1].set_title('Alignment Metrics', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    axes[2].scatter(t2d[:, 0], t2d[:, 1], alpha=0.6, s=40, c='steelblue',\n",
    "                    edgecolors='black', linewidth=0.5, label='Text')\n",
    "    axes[2].scatter(m2d[:, 0], m2d[:, 1], alpha=0.6, s=40, c='tomato',\n",
    "                    edgecolors='black', linewidth=0.5, label='Motion')\n",
    "    for i in range(min(20, N)):\n",
    "        axes[2].plot([t2d[i, 0], m2d[i, 0]], [t2d[i, 1], m2d[i, 1]],\n",
    "                     'gray', alpha=0.3, linewidth=0.8)\n",
    "    axes[2].set_title('t-SNE Embedding', fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    IPython_display.clear_output(wait=True)\n",
    "    IPython_display.display(fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "N_VIZ_LIVE = Config.N_VIZ_SAMPLES\n",
    "VIZ_EVERY = Config.VIZ_EVERY_N_EPOCHS\n",
    "viz_fnames_live = random.sample(train_fnames, min(N_VIZ_LIVE, len(train_fnames)))\n",
    "\n",
    "print('‚úì Dashboard ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. TRAINING LOOP avec MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "D√âBUT DE L'ENTRA√éNEMENT\n",
      "Motion Encoder : HUMANML3D\n",
      "Loss           : AlignedContrastive\n",
      "MLflow Run ID  : 57b6301f30a944f5a0ba29e4b1cc8108\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 235/451 [03:12<02:54,  1.24it/s, loss=1.8049]"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TRAINING LOOP avec MLflow TRACKING\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "EPOCHS = Config.EPOCHS\n",
    "PATIENCE = Config.PATIENCE\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "cos_correct_hist = []\n",
    "cos_wrong_hist = []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 1\n",
    "patience_count = 0\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "\n",
    "    # Logger la config\n",
    "    config_dict = {k: v for k, v in vars(Config).items()\n",
    "                   if not k.startswith('_') and not callable(v)}\n",
    "    mlflow.log_params(config_dict)\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "    print(f\"Motion Encoder : {Config.MOTION_MODEL_TYPE.upper()}\")\n",
    "    print(f\"Loss           : {'AlignedContrastive' if Config.USE_ALIGNED_LOSS else 'TEMOS/NTXent'}\")\n",
    "    print(f\"MLflow Run ID  : {run.info.run_id}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "        # ‚îÄ‚îÄ TRAIN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        text_encoder.train()\n",
    "        motion_encoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_metrics = {'contrastive': 0.0, 'mse': 0.0, 'uniformity': 0.0}\n",
    "\n",
    "        pbar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS}', leave=False)\n",
    "\n",
    "        for batch_idx, (text_batch, motion_batch, mask_batch) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_emb = encode_text(text_batch)\n",
    "            motion_emb = encode_motion(motion_batch, mask_batch)\n",
    "\n",
    "            loss, loss_dict = compute_loss(text_emb, motion_emb)\n",
    "\n",
    "            loss.backward()\n",
    "            if Config.GRAD_CLIP > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(text_encoder.parameters()) + list(motion_encoder.parameters()),\n",
    "                    Config.GRAD_CLIP\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss_dict['total']\n",
    "            for k in epoch_metrics:\n",
    "                if k in loss_dict:\n",
    "                    epoch_metrics[k] += loss_dict[k]\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % Config.LOG_EVERY_N_STEPS == 0:\n",
    "                mlflow.log_metric('train_loss_step', loss_dict['total'], step=global_step)\n",
    "                mlflow.log_metric('lr', optimizer.param_groups[0]['lr'], step=global_step)\n",
    "\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train)\n",
    "\n",
    "        # ‚îÄ‚îÄ VALIDATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        text_encoder.eval()\n",
    "        motion_encoder.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text_batch, motion_batch, mask_batch in val_loader:\n",
    "                text_emb = encode_text(text_batch)\n",
    "                motion_emb = encode_motion(motion_batch, mask_batch)\n",
    "                # Utiliser compute_loss pour g√©rer tuple/scalaire uniform√©ment\n",
    "                loss, _ = compute_loss(text_emb, motion_emb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val)\n",
    "\n",
    "        # ‚îÄ‚îÄ M√âTRIQUES AVANC√âES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        if epoch % VIZ_EVERY == 0 and Config.LOG_ADVANCED_METRICS:\n",
    "            with torch.no_grad():\n",
    "                all_text_embs, all_motion_embs = [], []\n",
    "\n",
    "                for text_batch, motion_batch, mask_batch in val_loader:\n",
    "                    t_emb = encode_text(text_batch)\n",
    "                    m_emb = encode_motion(motion_batch, mask_batch)\n",
    "                    all_text_embs.append(t_emb)\n",
    "                    all_motion_embs.append(m_emb)\n",
    "\n",
    "                all_text_cat = torch.cat(all_text_embs)\n",
    "                all_motion_cat = torch.cat(all_motion_embs)\n",
    "\n",
    "                adv_metrics = compute_advanced_metrics(all_text_cat, all_motion_cat)\n",
    "\n",
    "                # Logger dans MLflow (sans les alias courts pour √©viter doublons)\n",
    "                mlflow_adv = {\n",
    "                    k: v for k, v in adv_metrics.items()\n",
    "                    if k not in ('recall@1', 'recall@5')  # alias, d√©j√† logg√©s sous recall_at_X\n",
    "                }\n",
    "                mlflow.log_metrics({f'adv_{k}': v for k, v in mlflow_adv.items()}, step=epoch)\n",
    "\n",
    "                # Matrice de similarit√©\n",
    "                if Config.LOG_SIMILARITY_MATRIX:\n",
    "                    subset_size = min(64, len(all_text_cat))\n",
    "                    plot_and_log_similarity_matrix(\n",
    "                        all_text_cat[:subset_size],\n",
    "                        all_motion_cat[:subset_size],\n",
    "                        epoch\n",
    "                    )\n",
    "\n",
    "                print(f\"  AdvMetrics | centroid_dist={adv_metrics['centroid_dist']:.3f} \"\n",
    "                      f\"| gap={adv_metrics['gap']:.3f} \"\n",
    "                      f\"| R@1={adv_metrics['recall@1']:.3f} \"\n",
    "                      f\"| R@5={adv_metrics['recall@5']:.3f}\")\n",
    "\n",
    "        # ‚îÄ‚îÄ COSINE STATS POUR DASHBOARD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        if epoch % VIZ_EVERY == 0:\n",
    "            t_arr, m_arr, _ = get_live_embeddings(viz_fnames_live)\n",
    "            cos_c, cos_w = compute_cosine_stats(t_arr, m_arr)\n",
    "            cos_correct_hist.append(cos_c)\n",
    "            cos_wrong_hist.append(cos_w)\n",
    "        else:\n",
    "            cos_correct_hist.append(cos_correct_hist[-1] if cos_correct_hist else 0.0)\n",
    "            cos_wrong_hist.append(cos_wrong_hist[-1] if cos_wrong_hist else 0.0)\n",
    "\n",
    "        gap = cos_correct_hist[-1] - cos_wrong_hist[-1]\n",
    "\n",
    "        # ‚îÄ‚îÄ MLFLOW LOGGING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        mlflow.log_metric('train_loss', avg_train, step=epoch)\n",
    "        mlflow.log_metric('val_loss', avg_val, step=epoch)\n",
    "        mlflow.log_metric('cos_correct', cos_correct_hist[-1], step=epoch)\n",
    "        mlflow.log_metric('cos_incorrect', cos_wrong_hist[-1], step=epoch)\n",
    "        mlflow.log_metric('alignment_gap', gap, step=epoch)\n",
    "\n",
    "        # ‚îÄ‚îÄ EARLY STOPPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        if avg_val < best_val_loss - Config.MIN_DELTA:\n",
    "            best_val_loss = avg_val\n",
    "            best_epoch = epoch\n",
    "            patience_count = 0\n",
    "\n",
    "            torch.save(motion_encoder.state_dict(),\n",
    "                       pjoin(Config.SAVE_DIR, 'motion_encoder_best.pt'))\n",
    "            torch.save(text_encoder.state_dict(),\n",
    "                       pjoin(Config.SAVE_DIR, 'text_encoder_best.pt'))\n",
    "\n",
    "            mlflow.log_metric('best_val_loss', best_val_loss, step=epoch)\n",
    "        else:\n",
    "            patience_count += 1\n",
    "\n",
    "        # ‚îÄ‚îÄ DASHBOARD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        if epoch % VIZ_EVERY == 0:\n",
    "            render_dashboard(\n",
    "                train_losses, val_losses,\n",
    "                cos_correct_hist, cos_wrong_hist,\n",
    "                t_arr, m_arr, epoch, best_epoch\n",
    "            )\n",
    "\n",
    "            print(f'Ep {epoch:3d} | Train={avg_train:.4f} | Val={avg_val:.4f} '\n",
    "                  f'| Cos+={cos_correct_hist[-1]:.3f} Cos-={cos_wrong_hist[-1]:.3f} '\n",
    "                  f'Gap={gap:.3f} | Pat={patience_count}/{PATIENCE}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if patience_count >= PATIENCE:\n",
    "            print(f'\\nEarly stopping (ep {epoch}). Best val: {best_val_loss:.4f} (ep {best_epoch})')\n",
    "            break\n",
    "\n",
    "    # ‚îÄ‚îÄ FIN TRAINING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Recharger best model\n",
    "    motion_encoder.load_state_dict(\n",
    "        torch.load(pjoin(Config.SAVE_DIR, 'motion_encoder_best.pt'))\n",
    "    )\n",
    "    text_encoder.load_state_dict(\n",
    "        torch.load(pjoin(Config.SAVE_DIR, 'text_encoder_best.pt'))\n",
    "    )\n",
    "\n",
    "    if Config.LOG_MODEL:\n",
    "        mlflow.pytorch.log_model(motion_encoder, \"motion_encoder\")\n",
    "        mlflow.pytorch.log_model(text_encoder, \"text_encoder\")\n",
    "\n",
    "    print(f'\\n‚úì Training termin√©. Best model loaded (ep {best_epoch}, val={best_val_loss:.4f})')\n",
    "    print(f'\\nüìä MLflow Run ID: {run.info.run_id}')\n",
    "    print(f\"   View results: mlflow ui --backend-store-uri {Config.MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Graphiques finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "axes[0].plot(epochs, train_losses, label='Train', linewidth=2)\n",
    "axes[0].plot(epochs, val_losses, label='Val', linewidth=2)\n",
    "axes[0].axvline(best_epoch, color='red', linestyle='--', label=f'Best={best_epoch}', alpha=0.5)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Curves', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, cos_correct_hist, label='Correct', linewidth=2, color='green')\n",
    "axes[1].plot(epochs, cos_wrong_hist, label='Incorrect', linewidth=2, color='red')\n",
    "gap_hist = [c - w for c, w in zip(cos_correct_hist, cos_wrong_hist)]\n",
    "axes[1].plot(epochs, gap_hist, label='Gap', linestyle='--', linewidth=2, color='blue')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Cosine Similarity')\n",
    "axes[1].set_title('Alignment Metrics', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(pjoin(Config.SAVE_DIR, 'training_summary.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal gap: {gap_hist[-1]:.3f} (target: > 0.5)')\n",
    "print(f'Best val loss: {best_val_loss:.4f} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "torch.save(motion_encoder.state_dict(), pjoin(Config.SAVE_DIR, 'motion_encoder.pt'))\n",
    "torch.save(text_encoder.state_dict(), pjoin(Config.SAVE_DIR, 'text_encoder.pt'))\n",
    "print('‚úì Models saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "motion_encoder.load_state_dict(torch.load(pjoin(Config.SAVE_DIR, 'motion_encoder.pt')))\n",
    "text_encoder.load_state_dict(torch.load(pjoin(Config.SAVE_DIR, 'text_encoder.pt')))\n",
    "print('‚úì Models loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "TOP_K = 10\n",
    "TEST_DIR = pathlib.Path(pjoin(Config.DATA_ROOT, 'val/'))\n",
    "\n",
    "text_encoder.eval()\n",
    "motion_encoder.eval()\n",
    "\n",
    "submission_rows = []\n",
    "test_subdirs = sorted([d for d in TEST_DIR.iterdir() if d.is_dir()],\n",
    "                      key=lambda d: int(d.name))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sd in tqdm.tqdm(test_subdirs, desc='G√©n√©ration soumission'):\n",
    "        query_id = int(sd.name)\n",
    "\n",
    "        query_file = sd / 'query.txt' if (sd / 'query.txt').exists() else sd / 'text.txt'\n",
    "        query_text = query_file.read_text('utf-8').strip()\n",
    "\n",
    "        text_batch = tokenize_texts([query_text])\n",
    "        text_batch = {k: v.to(device) if v is not None else None for k, v in text_batch.items()}\n",
    "        text_emb = encode_text(text_batch)\n",
    "\n",
    "        candidate_files = sorted(sd.glob('motion_*.npy'))\n",
    "        candidate_ids = [int(f.stem.split('_')[-1]) for f in candidate_files]\n",
    "\n",
    "        motions, masks = [], []\n",
    "        for f in candidate_files:\n",
    "            m, msk = preprocess_motion(np.load(f), GLOBAL_MEAN, GLOBAL_STD)\n",
    "            motions.append(m)\n",
    "            masks.append(msk)\n",
    "\n",
    "        motion_batch = torch.stack(motions).to(device)\n",
    "        mask_batch = torch.stack(masks).to(device)\n",
    "        motion_embs = encode_motion(motion_batch, mask_batch)\n",
    "\n",
    "        sims = (text_emb @ motion_embs.T).squeeze(0)\n",
    "        k = min(TOP_K, len(candidate_ids))\n",
    "        top_idx = torch.topk(sims, k=k).indices.cpu().tolist()\n",
    "        top_ids = [candidate_ids[i] for i in top_idx]\n",
    "\n",
    "        row = {'query_id': query_id}\n",
    "        for rank, mid in enumerate(top_ids, start=1):\n",
    "            row[f'candidate_{rank}'] = mid\n",
    "        submission_rows.append(row)\n",
    "\n",
    "cols = ['query_id'] + [f'candidate_{k}' for k in range(1, TOP_K + 1)]\n",
    "submission_df = pd.DataFrame(submission_rows).reindex(columns=cols)\n",
    "submission_df.set_index('query_id', inplace=True)\n",
    "\n",
    "submission_df.to_csv('submission.csv')\n",
    "print('\\n‚úì submission.csv generated')\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(pjoin(str(TEST_DIR), 'gt.csv'))\n",
    "recall = eval_recall(gt_df, submission_df, verbose=True)\n",
    "print(f'\\nüéØ Weighted Recall: {recall:.4f}')\n",
    "\n",
    "# Log √† MLflow\n",
    "with mlflow.start_run(run_id=run.info.run_id):\n",
    "    mlflow.log_metric('final_recall', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
